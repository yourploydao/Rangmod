# ‚úÖ main.py (‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà merge ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ö‡∏ö Ollama-compatible)
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import requests

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á FastAPI app
app = FastAPI()

# ‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ frontend access ‡πÑ‡∏î‡πâ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà ["http://localhost:3000"] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡∏∞ merged model (‡∏ó‡∏µ‡πà merge LoRA ‡πÅ‡∏•‡πâ‡∏ß)
model_path = "./models/qwen-merged"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.float16  # ‡∏´‡∏£‡∏∑‡∏≠ "auto" ‡∏Å‡πá‡πÑ‡∏î‡πâ
).to("cpu")  # ‚úÖ ‡πÉ‡∏ä‡πâ CPU ‡πÅ‡∏ó‡∏ô .cuda()

# ‚úÖ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö input ‡∏à‡∏≤‡∏Å frontend
class ChatRequest(BaseModel):
    question: str

@app.post("/chat")
async def chat(request: ChatRequest):
    print(f"üì© ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {request.question}")

    try:
        contextRes = requests.post(
            "http://localhost:3000/api/ai/context",
            json={"query": request.question},
            timeout=10
        )
        contextRes.raise_for_status()
        contextData = contextRes.json()
        context = contextData.get("context", "")
    except Exception as e:
        print(f"‚ö†Ô∏è context error: {e}")
        context = ""

    context_part = f"Context:\n{context.strip()}\n\n" if context else ""

    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏≠‡∏û‡∏±‡∏Å‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

{context_part}Q: {request.question}
A:"""

    try:
        inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
        input_len = inputs.input_ids.shape[-1]

        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )

        generated_tokens = outputs[0][input_len:]
        answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print(f"‚úÖ ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö: {answer}")
        return { "answer": answer or "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ" }
    
    except Exception as e:
        print(f"‚ùå generate error: {e}")
        return { "answer": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°" }