# ‚úÖ main.py (‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà merge ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ö‡∏ö Ollama-compatible)
from fastapi import FastAPI
from pydantic import BaseModel
import requests
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ‡πÇ‡∏´‡∏•‡∏î model ‡∏ó‡∏µ‡πà merge LoRA ‡πÅ‡∏•‡πâ‡∏ß
model_path = "./models/qwen-merged"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

app = FastAPI()

class ChatInput(BaseModel):
    question: str

@app.post("/chat")
async def chat(input: ChatInput):
    print("‚úÖ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:", input.question)
    # ‚úÖ 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å context ‡∏à‡∏≤‡∏Å vector search API
    try:
        rag_resp = requests.post("http://localhost:3000/api/ai/context", json={"query": input.question})
        rag_context = rag_resp.json().get("context", "")
    except Exception:
        rag_context = ""

    # ‚úÖ 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡πÅ‡∏ö‡∏ö plain
    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏≠‡∏û‡∏±‡∏Å‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

{rag_context}

Q: {input.question}
A:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_length = inputs.input_ids.shape[-1]
    print("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á generate...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

    answer = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()
    return { "answer": answer }

